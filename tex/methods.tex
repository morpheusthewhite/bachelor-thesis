\chapter{Metodi}
\label{section:methods}

Per lo sviluppo di questa applicazione si è scelto il linguaggio Python per la sua semplicità 
e immediatezza; ugualmente si sarebbe potuto optare per linguaggi come C++ o Java senza in alcun 
modo dover modificare la logica sviluppata.

\section{Individuazione degli studenti}
\label{section:methods_face_detection}

Il primo \textit{step} che è necessario affrontare nello sviluppo di questa applicazione
è la \textit{face detection} per individuare il numero di studenti presenti nelle immagini 
fornite. 

Si è deciso di utilizzare, per ovviare a questa problematica, alla libreria offerta da OpenCV, 
la quale offre inoltre dei classificatori pre-trainati (che utilizzano gli algoritmi introdotti 
in \ref{section:viola_jones}).

\begin{figure}
    \begin{small}
        \begin{center}
            \includegraphics[width=0.20\textwidth]{opencv.png}
        \end{center}
        \caption{Il logo di OpenCV, una libreria molto utilizzata per la \textit{computer vision} 
            \cite{opencv}}
        \label{fig:opencv}
    \end{small}
\end{figure}

Tra questi sono stati scelti un modello per il riconoscimento dei volti visti frontalmente ed 
uno per il riconoscimento di quelli visti di profilo, rispettivamente il \lstinline{haarcascade_frontalface_default.xml} 
e \lstinline{haarcascade_profileface.xml}.

\begin{minted}{python}
face_cascade_frontal = cv.CascadeClassifier(os.path.join(CLASSIFIERS_FOLDER, CLASSIFIER_FILENAME_FRONTAL))
face_cascade_profile = cv.CascadeClassifier(os.path.join(CLASSIFIERS_FOLDER, CLASSIFIER_FILENAME_PROFILE))
\end{minted}

Le facce riconosciute sono uguali all'unione di quelle riconosciute dai singoli classificatori

\begin{minted}{python}
# detect faces
faces_profile = face_cascade_profile.detectMultiScale(img_gray, 1.3, 5)
faces_frontal = face_cascade_frontal.detectMultiScale(img_gray, 1.3, 5)

# merge faces detected
faces = list(faces_profile) + list(faces_frontal)
\end{minted} 

Per evitare che delle facce, riconosciute da entrambi i modelli, siano contate più di una volta, è stato poi
inserito un ulteriore controllo con una soglia sull'area di overlapping dei rettangoli che contengono i visi:
in questo modo se l'area in comune tra due di essi rapportata all'area di uno dei due è maggiore di una certa 
soglia, uno dei due è scartato.

\begin{minted}{python}
valid_faces = []
# filter overlapping faces
for i in range(len(faces)):
    (x1, y1, w1, h1) = faces[i]
    other_faces = faces[i + 1:]

    valid_faces.append(faces[i])

    for (x2, y2, w2, h2) in other_faces:
        overlapping_area = get_overlapping_area(x1, y1, w1, h1, x2, y2, w2, h2)

        if overlapping_area / w1 * h1 > OVERLAPPING_THRESHOLD:
            valid_faces.pop()
            break
\end{minted}
\noindent
avendo precedentemente definito

\begin{minted}{python}
OVERLAPPING_THRESHOLD = 0.9
\end{minted} 

\noindent
che, in seguito a diversi test, si è rivelato un valore valido per il rilevamento 
dei rettangoli che si sovrapponevano, e

\begin{minted}{python}
def get_overlapping_area(x1, y1, w1, h1, x2, y2, w2, h2):
    """
    Calculates the overlapping area given the two rectangles respectively defined by (x1, y1, w1, h1)
    and (x2, y2, w2, h2)
    """
\end{minted} 

\section{Il modello di regressione}
\label{section:methods_ml}

\subsection{Struttura della rete}
\label{section:methods_ml_structure}

Per stimare il numero di studenti viene utilizzato un modello di rete neurale molto semplice:
essa consiste in un layer di input, due layer nascosti, con funzione di 
attivazione la ReLU \ref{eq:relu}, ed un layer di output.
Per l'implementazione si utilizza Keras, una libreria che offre un interfaccia 
immediata e completa per il machine learning in Python.

\begin{minted}{python}
# define base model
def baseline_model(inputs=9):
    # create model
    model = Sequential()
    model.add(Dense(inputs, input_dim=inputs, kernel_initializer='normal', activation='relu'))
    model.add(Dense(12, kernel_initializer='normal', activation='relu'))
    model.add(Dense(12, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))

    # Compile model
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r_square, rmse])

    return model
\end{minted} 

Come funzione di loss il modello utilizzerà l'MSE (Mean Square Error, o errore quadratico medio) 
definito come (essendo $e_j$ lo scostamento nella stima j-esima di n)\cite{Botchkarev2018}

\begin{equation}
    MSE = \frac{1}{n} \sum_{j=1}^{n} e_j^2
    \label{eq:mse}
\end{equation}

e come metriche ulteriori il RMSE (Root Mean Square Error), che corrisponde alla radice quadrata 
di \ref{eq:mse} e \textit{R squared}, calcolato come \cite{RePEc:pra:mprapa:84722}

\begin{equation}
    R^2 = 1 - \frac{SSE}{SST}
    \label{eq:r2}
\end{equation}
\noindent
con $SSE$ somma dei quadrati degli errori e $SST$ somma dei quadrati degli scostamenti dei valori 
reali dalla media.

Di particolare interesse è la prima metrica, in quanto risulta indicatrice del numero di studenti 
di differenza tra la stima effettuata ed i dati analizzati.

\begin{minted}{python}
def rmse(y_true, y_pred):
    from keras import backend
    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))

def r_square(y_true, y_pred):
    from keras import backend as K
    SS_res = K.sum(K.square(y_true - y_pred))
    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )
\end{minted} 

Il regressore viene quindi creato come specificato in \lstinline{baseline_model}

\begin{minted}{python}
self.estimator = KerasRegressor(build_fn=baseline_model, inputs=len(self.X.columns), epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=1)
\end{minted}

avendo precedentemente definito

\begin{minted}{python}
N_EPOCHS = 100
BATCH_SIZE = 5
\end{minted}

\subsection{Training} % (fold)
\label{sub:methods_ml_training}

Il dataset utilizzato per il training (generato artificialmente) consiste in una serie di entry del 
tipo

\begin{minted}{json}
{"hour": 18, "students": 100, "day": "thursday", "subject": "CAL"}
\end{minted} 

(sono stati inseriti anche il numero di studenti poichè generare un dataset con delle label 
associate a foto sarebbe stato molto dispendioso e non necessario, visto che l'implementazione
scelta dell'applicazione permette sia di partire da foto con informazioni associate sia direttamente 
da entry come quella appena mostrata, essendo la parte di face detection separata da quella di 
machine learning).

La vera fase di training quindi viene effettuata nella seguente funzione, definita per la classe 
\lstinline{StudentsEstimator}, un wrapper per il modello di regressione e che effettua anche la fase
di encoding dei dati

\begin{minted}{python}
def train(self, early_stopping=True):
    # train model
    callbacks = []

    if early_stopping:
        early_stopping = EarlyStopping(monitor='val_loss', patience=10)
        callbacks.append(early_stopping)

    history = self.estimator.fit(self.X, self.Y, validation_split=VALIDATION_FRACTION, callbacks=callbacks)
\end{minted} 
\noindent
con \lstinline{VALIDATION_FRACTION} precedentemente definita pari a $0.1$. 

Se non specificato, viene effettuato l'\textit{early stopping}, cioè il training viene interrotto 
nel momento in cui per un numero di epoche consecutive pari alla \lstinline{patience} la loss nei test 
cresce anziché diminuire (questo procedimento viene solitamente aggiunto per evitare \textit{overfitting} 
sui dati, che cioè la rete diventi un modello adatto a rappresentare ottimamente solo i dati di training
e performi molto male sui restanti). 

Gli score e le metriche precedentemente introdotti verranno poi estrapolati dalla \lstinline{history} 
restituita da \lstinline{estimator.fit()}.

A questo punto vengono effettuate una o più stime attraverso il modello costruito, dopo aver codificato i 
dati in modo coerente rispetto al training

\begin{minted}{python}
def predict(self, prediction_data):
    # encode data before prediction
    dp = pandas.DataFrame(prediction_data)
    X = self.transform_data(dp)

    predictions_float = self.estimator.predict(X)
\end{minted} 

Si dispone quindi di entry identiche nella forma a quelle utilizzate per il training, con la 
differenza che le prime sono ottenute attraverso delle stime. Si passa quindi all'ultima parte 
dell'applicazione, incentrata sull'allocazione dei gruppi di studenti. 

\begin{figure}
    \begin{small}
        \begin{center}
            \includegraphics[width=0.50\textwidth]{keras.png}
        \end{center}
        \caption{Keras è una API ad alto livello per il machine learning in Python
            e che si può basare su TensorFlow, CNTK e Theano \cite{keras}}
        \label{fig:}
    \end{small}
\end{figure}

\newpage

\section{Allocazione degli studenti}
\label{section:methods_allocation}

\subsection{Formulazione multiobiettivo}
\label{section:allocation_formulation}

Se rappresento con $S = \{s_1, \dots, s_n\}$ l'insieme degli $n$  gruppi di studenti in un certo giorno e ora e con 
$R = \{r_1, \dots, r_m\}$ l'insieme
delle $m$  aule disponibili, definite delle variabili decisionali $y_{ij}$ (che assumono valore 1 se il 
gruppo di studenti $i$ è assegnato all'aula $j$, 0 altrimenti), allora le $n$  funzioni obiettivo 
sono le seguenti 

\begin{equation}
    \max \bigg( \sum_{j = 1}^{m}\frac{s_i}{r_j} \cdot y_{ij} \bigg) \quad \forall i \in \{1, \dots, n\}
    \label{eq:objs}
\end{equation}

Con vincoli

\begin{itemize}
    \item Gruppi di studenti sono assegnati ad aule con capienza maggiore uguale al loro numero

    \begin{equation}
        s_i \leq \sum_{j=1}^{m} y_{ij} \cdot r_j \quad \forall i \in \{1, \dots, n\}
        \label{eq:constraint_capacity}
    \end{equation}

    \item Ogni aula è assegnata al massimo ad un gruppo di studenti
    
    \begin{equation}
        \sum_{i=1}^{n} y_{ij} \leq 1 \quad \forall j \in \{1, \dots, m\}
        \label{eq:constraint_rooms}
    \end{equation}

    \item Ogni gruppo di studenti è assegnato esattamente ad un'aula
    
    \begin{equation}
        \sum_{j=1}^{m} y_{ij} = 1 \quad \forall i \in \{1, \dots, n\}
        \label{eq:constraint_students}
    \end{equation}
    
    \item Le variabili sono binarie
    
    \begin{equation}
        y_{ij} \in \{0,1\} \quad \forall i \in \{1,\dots, n\}, \forall j \in \{1, \dots, m\}
        \label{eq:_constraint_binary}
    \end{equation}
    
\end{itemize}

Le soluzioni efficienti di questo problema possono essere calcolate con il metodo delle 
$\epsilon$-constraints (\ref{section:epsilon_constraints}).

\subsection{Scelta della soluzione}
\label{section:sol_choice}

Delle diverse soluzioni efficienti interessa però selezionarne un sottoinsieme finito:
si richiede infatti che nel caso di due ottimi di Pareto venga scelto quello che lascia 
un numero di posti vuoti $v_i$ maggiore se $s_i$ è maggiore, i. e., dati ad esempio i seguenti gruppi di studenti e di aule

\begin{flalign*}
    S = \{s_1, s_2\}\\
    R = \{r_1, r_2, r_3\} \\
\end{flalign*}

con $r_1 < s_1 < s_2 < r_2 < r_3$ allora la soluzione preferita sarà quella con $y_{12} = 1$, $y_{23}=1$ e le restanti variabili nulle.
Si richiede cioè che, dati $p$ e $q$, $y_{ps}=1$ e $y_{qt}=1$, $s_p \leq s_q \implies r_s \leq r_t$.

\subparagraph{Un esempio}

Dato il seguente problema

\begin{center}
\begin{tabular}[c]{|c|c|c|}
    \hline
    \multirow{2}{*}{Studenti} & $\mathbf{s_1}$  & $\mathbf{s_2}$ \\
    \cline{2-3}
    & 36 & 38 \\
    \hline
\end{tabular}
\quad \quad
\begin{tabular}[c]{|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Aule} & $\mathbf{r_1}$ & $\mathbf{r_2}$ & $\mathbf{r_3}$ & $\mathbf{r_4}$ \\
    \cline{2-5}
    & 30 & 40 & 50 & 60 \\
    \hline
\end{tabular}
\end{center}

Tra i due ottimi di Pareto viene quindi preferita quella che associa $s_1$ a $r_2$ e $s_2$ a $r_3$.

\subsection{Formulazione singolo obiettivo}

Le $n$ funzioni obiettivo devono essere quindi trasformate in una sola per poter risolvere il problema:
utilizzando il metodo dei pesi (Capitolo~\ref{section:weight_method}), attraverso un vettore di coefficienti,
rilasciando la richiesta non necessaria che la somma delle componenti sia pari a 1
(\cite{Marler2009}, pag. 854)

\begin{equation*}
    \mathbf{\lambda} = \begin{bmatrix}
        \lambda_1 \\
        \vdots \\
        \lambda_n \\
    \end{bmatrix}
    \label{eq:}
\end{equation*}
 
Le funzioni obiettivo diventano

\begin{equation}
    \max \bigg( \sum_{i=1}^{n} \lambda_{i} \cdot \sum_{j = 1}^{m}\frac{s_i}{r_j} \cdot y_{ij}\bigg)
    \label{eq:}
\end{equation}

\subsubsection{Scelta dei pesi}

Considerando gli $s_i$ in ordine crescente, il valore dei pesi che sintetizza la richiesta 
nel Capitolo~\ref{section:sol_choice} si verifica essere

\begin{equation}
    \lambda_i = \prod_{k=1,\\ k\neq i}^{n} (s_k + \epsilon (i -1))
    \label{eq:}
\end{equation}

\noindent
con $\epsilon > 0$ piccolo a piacere.

La funzione da massimizzare diventa quindi

\begin{equation}
    \max \bigg( \sum_{i=1}^{n} \prod_{k=1,\\ k\neq i}^{n} (s_k - \epsilon (i- 1)) \cdot \sum_{j = 1}^{m}\frac{s_i}{r_j} \cdot y_{ij}\bigg)
    \label{eq:single_obj}
\end{equation}

\begin{proof}
    Bisogna dimostrare che, dato il problema definito da (\ref{eq:single_obj})
    la soluzione ottima rispetta la richiesta in Capitolo~\ref{section:sol_choice}.
    
    Si considerino i gruppi di studenti ordinati in modo crescente e, per 
    semplificare la notazione, anche la capacità delle aule;
    si consideri inoltre il caso in cui il problema sia ammissibile e $r_i < s_1, \dots, s_n \leq r_{i+1}$ ($r_i$ potrebbe 
    anche non esistere): la richiesta non è restrittiva, in quanto la dimostrazione negli 
    altri casi è analoga.

    Sia $\bar{r}_i$ la capacità dell'aula associata nella soluzione 
    ottima al gruppo di studenti $i$ e con $\tilde{r}_i$ la capacità associata in una 
    qualunque soluzione ammissibile al gruppo di studenti $i$.

    E' quindi necessario provare che 
    
    \begin{equation}
        s_q \geq s_p \implies \bar{r}_q \geq \bar{r}_p    
        \label{eq:}
    \end{equation}

    \noindent
    (essendo i gruppi di studenti ordinati in senso crescente risulterà inoltre $p \leq q$)
    
    Essendo soluzione ottima del problema, vale
    
    \begin{equation*}
        \lambda_1 \frac{s_1}{\bar{r}_1} + \dots + \lambda_{n} \frac{s_{n}}{\bar{r}_{n}} \geq
        \lambda_1 \frac{s_1}{\tilde{r}_1} + \dots + \lambda_{n} \frac{s_{n}}{\tilde{r}_{n}}
        \label{eq:}
    \end{equation*}
    
    \noindent
    per ogni scelta di $\tilde{r}_i$ che risulti ammissibile per il problema; in particolare, 
    si consideri quella in cui $\tilde{r}_i = \bar{r}_i \quad \forall i \in \{1, \dots, n\}, i \neq p, q$,
    in cui cioè la scelta delle associazioni corrisponda sempre tranne per i gruppi di studenti 
    $s_p$ e $s_q$.
    
    La disuguaglianza diventa quindi
    
    \begin{equation*}
        \lambda_p \frac{s_p}{\bar{r}_p} + \lambda_{q} \frac{s_{q}}{\bar{r}_{q}} \geq
        \lambda_1 \frac{s_p}{\tilde{r}_p} + \lambda_{q} \frac{s_{q}}{\tilde{r}_{q}}
        \label{eq:}
    \end{equation*}
    
    Si supponga inoltre il caso (ammissibile) in cui $\tilde{r}_p = \bar{r}_q$ e $\tilde{r}_q = \bar{r}_p$  
    
    \begin{equation*}
        \lambda_p \frac{s_p}{\bar{r}_p} + \lambda_{q} \frac{s_{q}}{\bar{r}_{q}} \geq
        \lambda_1 \frac{s_p}{\bar{r}_q} + \lambda_{q} \frac{s_{q}}{\bar{r}_{p}}
        \label{eq:}
    \end{equation*}
    \begin{equation*}
        \lambda_p s_p \bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q}\bigg) \geq 
        \lambda_q s_q \bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q} \bigg)
        \label{eq:proof}
    \end{equation*}
    \begin{equation*}
        \frac{\lambda_p}{\lambda_q}  \bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q}\bigg) \geq 
        \frac{s_q}{s_p} \bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q} \bigg)
        \label{eq:proof}
    \end{equation*}
    
    Adesso, supponendo PA che $\bar{r}_p \geq \bar{r}_q$ e quindi $\bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q} \bigg) \leq 0$
    
    \begin{equation}
        \frac{\lambda_p}{\lambda_q} \leq 
        \frac{s_q}{s_p}
        \label{eq:proof_dis}
    \end{equation}
    
    Con (dato $s_p \leq s_q$) 
    
    \begin{equation*}
        \frac{\lambda_p}{\lambda_q} = \frac{(s_1 - \epsilon(p-1))\dots(s_{p-1} - \epsilon(p-1))(s_{p+1} - \epsilon(p-1))
        \dots(s_n - \epsilon(p-1))}
        {(s_1 - \epsilon(q-1))\dots(s_{q-1} - \epsilon(q-1))(s_{q+1} - \epsilon(q-1))\dots(s_n - \epsilon(q-1))}
        \label{eq:}
    \end{equation*}
    \begin{equation*}
        \geq \frac{(s_q - \epsilon(p-1))}
        {(s_p - \epsilon(q-1))}
        \label{eq:}
    \end{equation*}
    
    \noindent
    essendo i fattori $\frac{(s_i - \epsilon(p-i))}{(s_{i} - \epsilon(q-i))} \geq 1$ (poichè $p \leq q$).
    
    Quindi (\ref{eq:proof_dis}) diventa
    
    \begin{equation*}
        \frac{(s_q - \epsilon(p-1))}
        {(s_p - \epsilon(q-1))} \leq 
        \frac{s_q}{s_p}
        \label{eq:proof}
    \end{equation*}
    \begin{equation*}
        \frac{(s_q - \epsilon(p-1))}
        {(s_p - \epsilon(q-1))}
        \cdot
        \frac{(s_q - \epsilon(q-1))}
        {(s_q - \epsilon(q-1))} \leq 
        \frac{s_q}{s_p}
        \label{eq:proof}
    \end{equation*}
    \begin{equation*}
        \frac{(s_q - \epsilon(p-1))}
        {(s_q - \epsilon(q-1))}
        \cdot
        \frac{(s_q - \epsilon(q-1))}
        {(s_p - \epsilon(q-1))} \leq 
        \frac{s_q}{s_p}
        \label{eq:proof}
    \end{equation*}
    
    Si vede facilmente che anche $
    \frac{(s_q - \epsilon(p-1))}
    {(s_q - \epsilon(q-1))} \geq 1$, quindi
    
    \begin{equation*}
        \frac{(s_q - \epsilon(q-1))}
        {(s_p - \epsilon(q-1))} \leq 
        \frac{s_q}{s_p}
        \label{eq:proof}
    \end{equation*}
    
    \noindent
    che risulta falsa per $s_q \geq s_p$. 
    
    Quindi deve essere $\bigg( \frac{1}{\bar{r}_p} - \frac{1}{\bar{r}_q} \bigg) \geq 0$, cioè
    
    \begin{equation}
        \frac{1}{\bar{r}_p} \geq \frac{1}{\bar{r}_q} \implies \bar{r}_q \geq \bar{r}_p
        \label{eq:}
    \end{equation}
    
\end{proof} 
    
\subsection{Implementazione}
\label{section:allocation_implementation}

Per la risoluzione del problema formulato verrà utilizzata la libreria \lstinline{cplex}  di Python la quale 
offre la possibilità di utilizzare da codice esterno le funzionalità di IBM CPLEX, un software 
incentrato sulla risoluzione di problemi di programmazione matematica.

Dopo aver creato ed inizializzato il modello

\begin{minted}{python}
# create CPLEX object
cplex_model = cplex.Cplex()
cplex_model.set_log_stream(None)
cplex_model.set_warning_stream(None)
cplex_model.set_results_stream(None)
\end{minted} 

\noindent
si passa alla definizione della matrice delle variabili decisionali e alla funzione di costo

\begin{minted}{python}
# create the vector of the decision variables
y_vars_name = ["y" + str(i) + str(j) for i in range(n_students) for j in range(n_rooms)]
cplex_model.variables.add(names=y_vars_name, types=[cplex_model.variables.type.binary] * n_students * n_rooms, obj=cost_vector)

# we want to maximize
cplex_model.objective.set_sense(cplex_model.objective.sense.maximize)
\end{minted} 

\noindent
ed all'aggiunta dei vincoli, quello sull'assegnazione degli studenti 
in aule con capacità maggiore (\ref{eq:constraint_capacity}),

\begin{minted}{python}
cplex_model.linear_constraints.add(
    lin_expr=[cplex.SparsePair(ind=["y" + str(i) + str(j)], val=[lessons_students[i]])
                for i in range(n_students) for j in range(n_rooms)],
    senses=["L"] * n_rooms * n_students,
    rhs=[rooms_seats[j] for i in range(n_students) for j in range(n_rooms)],
    range_values=[0] * n_rooms * n_students)
\end{minted} 

\noindent
quello sull'unicità dell'assegnazione delle aule (\ref{eq:constraint_rooms})

\begin{minted}{python}
cplex_model.linear_constraints.add(
    lin_expr=[cplex.SparsePair(ind=["y" + str(i) + str(j) for i in range(n_students)], val=[1] * n_students)
                for j in range(n_rooms)],
    senses=["L"] * n_rooms,
    rhs=[1] * n_rooms,
    range_values=[0] * n_rooms)
\end{minted} 

\noindent
e quello sull'unicità dell'assegnazione degli studenti (\ref{eq:constraint_students})

\begin{minted}{python}
cplex_model.linear_constraints.add(
    lin_expr=[cplex.SparsePair(ind=["y" + str(i) + str(j) for j in range(n_rooms)], val=[1] * n_rooms)
                for i in range(n_students)],
    senses=["E"] * n_students,
    rhs=[1] * n_students,
    range_values=[0] * n_students)
\end{minted} 

Viene quindi risolto il problema

\begin{minted}{python}
try:
    cplex_model.solve()
except cplex.CplexError as exc:
    print(exc)
    return

# get decision variables values
solution = cplex_model.solution
var_values = solution.get_values(0, cplex_model.variables.get_num() - 1)
\end{minted} 

